{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import Levenshtein\n",
    "\n",
    "df = pd.read_csv(r'D:\\Users\\peter\\PythonProjects\\PandasTest\\data\\Week 05 Wed.csv')\n",
    "\n",
    "#df.info()\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column to dtype 'datatime'\n",
    "df['Submitted at'] = pd.to_datetime(df['Submitted at'])\n",
    "\n",
    "# Convert the column to dtype 'object' to handle strings properly\n",
    "df['surveyid'] = df['surveyid'].astype('object')\n",
    "\n",
    "# Replace 'Untitled multiple choice field' with 'More Info' in all column names\n",
    "df.columns = df.columns.str.replace(r'Untitled multiple choice field', 'More Info')\n",
    "#df\n",
    "\n",
    "# Remove the 'Submitted at' column...\n",
    "column = df.pop('Submitted at')\n",
    "\n",
    "# Insert 'Submitted at' column at index position 6\n",
    "df.insert(6, 'Submitted at', column)\n",
    "\n",
    "# Insert two new columns that we need later\n",
    "df.insert(loc=6, column='Response_Count', value='0')\n",
    "df.insert(loc=0, column='Is_Duplicate', value=False)\n",
    "\n",
    "#df.info()\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows when 'can_id' is NaN\n",
    "\n",
    "df.dropna(subset=['can_id'], inplace=True)\n",
    "\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill 'surveyid' with the specified string\n",
    "# How do we pass this string into the DataFrame script?\n",
    "\n",
    "df['surveyid'] = df['surveyid'].fillna('Week 05 Wed')\n",
    "\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Email validation pattern\n",
    "email_pattern = re.compile(r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\")\n",
    "\n",
    "# Common email domains\n",
    "common_domains = {\n",
    "    \"350seattle.org\": \"350seattle.org\",\n",
    "    \"aol.com\": \"aol.com\",\n",
    "    \"comcast.net\": \"comcast.net\",\n",
    "    \"duck.com\": \"duck.com\",\n",
    "    \"fastmail.com\": \"fastmail.com\",\n",
    "    \"frontier.com\": \"frontier.com\",\n",
    "    \"gmail.com\": \"gmail.com\",\n",
    "    \"hotmail.com\": \"hotmail.com\",\n",
    "    \"icloud.com\": \"icloud.com\",\n",
    "    \"mac.com\": \"mac.com\",\n",
    "    \"me.com\": \"me.com\",\n",
    "    \"msn.com\": \"msn.com\",\n",
    "    \"outlook.com\": \"outlook.com\",\n",
    "    \"pobox.com\": \"pobox.com\",\n",
    "    \"proton.me\": \"proton.me\",\n",
    "    \"protonmail.com\": \"protonmail.com\",\n",
    "    \"uw.edu\": \"uw.edu\",\n",
    "    \"yahoo.com\": \"yahoo.com\"\n",
    "}\n",
    "\n",
    "# Common TLD misspellings\n",
    "tld_corrections = {\n",
    "    'com': ['copm', 'co', 'cpm', 'comm', 'copm'],\n",
    "    'org': ['ogr', 'orgg', 'or'],\n",
    "    'net': ['nett', 'ne', 'nte'],\n",
    "    'gov': ['gvo', 'govv', 'go'],\n",
    "    'edu': ['ed', 'edd', 'eud']\n",
    "}\n",
    "\n",
    "# Function to correct common TLD misspellings\n",
    "def correct_tld(domain):\n",
    "    tld = domain.split('.')[-1]\n",
    "    base_domain = '.'.join(domain.split('.')[:-1])\n",
    "    for correct_tld, misspellings in tld_corrections.items():\n",
    "        if tld in misspellings:\n",
    "            return f\"{base_domain}.{correct_tld}\"\n",
    "    return domain\n",
    "\n",
    "# Function to validate and correct email addresses\n",
    "def validate_and_correct_email(email):\n",
    "    if isinstance(email, str) and email_pattern.match(email):\n",
    "        domain = email.split('@')[1]\n",
    "        corrected_domain = correct_tld(domain)\n",
    "        if corrected_domain in common_domains:\n",
    "            return email.split('@')[0] + '@' + corrected_domain\n",
    "        else:\n",
    "            # Find the closest match if the domain is incorrect\n",
    "            corrected_domain = min(common_domains.keys(), key=lambda k: Levenshtein.distance(corrected_domain, k))\n",
    "            return email.split('@')[0] + '@' + corrected_domain\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Convert all email values to strings, replacing NaN with empty string\n",
    "df['Email'] = df['Email'].astype(str).fillna('')\n",
    "\n",
    "# Validate and correct email addresses\n",
    "df['Email'] = df['Email'].apply(validate_and_correct_email)\n",
    "\n",
    "#df.info()\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup df to compare and fill empty 'Email' and 'Name' values based on 'Respondent ID'\n",
    "\n",
    "# Identify duplicates in 'Respondent ID' with 'True' in 'Is_Duplicate' column\n",
    "df['Is_Duplicate'] = df['Respondent ID'].duplicated(keep=False)\n",
    "\n",
    "# Sort by 'Respondent ID' and reset index\n",
    "sorted_df = df.sort_values(by=['Respondent ID'], ascending=[True])\n",
    "sorted_df = sorted_df.reset_index(drop=True) # Reset index\n",
    "\n",
    "df = sorted_df\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_6376\\1744984796.py:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  RespondentID_groups = grouped.apply(lambda x: fill_columns(x.reset_index(drop=True))).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Identify the duplicates for 'Respondent ID'\n",
    "duplicate_respondents = df[df.duplicated('Respondent ID', keep=False)]\n",
    "\n",
    "# Group by 'Respondent ID' to handle each set of duplicates\n",
    "grouped = duplicate_respondents.groupby('Respondent ID')\n",
    "\n",
    "# Function to fill empty values in the 'Email' and 'Name' columns for each group\n",
    "def fill_columns(group):\n",
    "    group = group.copy()  # Avoid SettingWithCopyWarning\n",
    "    \n",
    "    # Find non-NaN values if they exist\n",
    "    non_nan_email = group['Email'].dropna().iloc[0] if not group['Email'].dropna().empty else None\n",
    "    non_nan_name = group['Name'].dropna().iloc[0] if not group['Name'].dropna().empty else None\n",
    "    \n",
    "    # Fill NaN values with non-NaN values found within the group\n",
    "    if non_nan_email:\n",
    "        group['Email'] = group['Email'].fillna(non_nan_email)\n",
    "    if non_nan_name:\n",
    "        group['Name'] = group['Name'].fillna(non_nan_name)\n",
    "    \n",
    "    return group\n",
    "\n",
    "# Apply the function to each group and reset the index\n",
    "RespondentID_groups = grouped.apply(lambda x: fill_columns(x.reset_index(drop=True))).reset_index(drop=True)\n",
    "\n",
    "# Combine the manipulated groups with the original DataFrame\n",
    "# First, drop the rows that were manipulated from the original DataFrame\n",
    "df_non_duplicates = df[~df['Respondent ID'].isin(RespondentID_groups['Respondent ID'])]\n",
    "\n",
    "# Concatenate the manipulated groups with the non-duplicate rows\n",
    "final_df = pd.concat([df_non_duplicates, RespondentID_groups]).sort_values(by='Respondent ID').reset_index(drop=True)\n",
    "\n",
    "df = final_df\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup df to compare and fill empty 'Email' and 'Name' values based on 'can_id'\n",
    "\n",
    "# Identify duplicates in 'can_id' with 'True' in 'Is_Duplicate' column\n",
    "df['Is_Duplicate'] = df['can_id'].duplicated(keep=False)\n",
    "\n",
    "# Sort by 'can_id' and reset index\n",
    "sorted_df = df.sort_values(by=['can_id'], ascending=[True])\n",
    "sorted_df = sorted_df.reset_index(drop=True) # Reset index\n",
    "\n",
    "df = sorted_df\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_6376\\3683480017.py:29: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  can_id_groups_filled = can_id_groups.apply(lambda x: fill_columns(x.reset_index(drop=True))).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Identify the duplicates for 'can_id'\n",
    "duplicate_candidates = df[df.duplicated('can_id', keep=False)]\n",
    "\n",
    "# Group by 'can_id' to handle each set of duplicates\n",
    "can_id_groups = duplicate_candidates.groupby('can_id')\n",
    "\n",
    "# Function to fill 'Email' and 'Name' columns for each group\n",
    "def fill_columns(group):\n",
    "    group = group.copy()  # Avoid SettingWithCopyWarning\n",
    "    \n",
    "    # Find non-NaN values if they exist\n",
    "    non_nan_email = group['Email'].dropna().unique()\n",
    "    non_nan_name = group['Name'].dropna().unique()\n",
    "    \n",
    "    # Check for the unique non-NaN values for 'Email' or 'Name'\n",
    "    if len(non_nan_email) == 1 and len(non_nan_name) == 1:\n",
    "        group['Email'] = group['Email'].fillna(non_nan_email[0])\n",
    "        group['Name'] = group['Name'].fillna(non_nan_name[0])\n",
    "\n",
    "    elif len(non_nan_email) == 1 and len(non_nan_name) == 0:\n",
    "        group['Email'] = group['Email'].fillna(non_nan_email[0])\n",
    "\n",
    "    elif len(non_nan_email) == 0 and len(non_nan_name) == 1:\n",
    "        group['Name'] = group['Name'].fillna(non_nan_name[0])\n",
    "    \n",
    "    return group\n",
    "\n",
    "# Apply the function to each group and reset the index\n",
    "can_id_groups_filled = can_id_groups.apply(lambda x: fill_columns(x.reset_index(drop=True))).reset_index(drop=True)\n",
    "\n",
    "# Combine the filled groups with the original DataFrame\n",
    "# First, drop the rows that were manipulated from the original DataFrame\n",
    "df_non_duplicates = df[~df['can_id'].isin(can_id_groups_filled['can_id'])]\n",
    "\n",
    "# Concatenate the filled groups with the non-duplicate rows\n",
    "final_df = pd.concat([df_non_duplicates, can_id_groups_filled]).sort_values(by='can_id').reset_index(drop=True)\n",
    "\n",
    "df = final_df\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fillna to replace the remaining NaN 'Email' values with 'can_id'\n",
    "df['Email'] = df['Email'].fillna(df['can_id'])\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup df for comparing and updating empty column values for duplicate rows\n",
    "\n",
    "# Identify duplicates in 'can_id' with 'True' in 'Is_Duplicate' column\n",
    "df['Is_Duplicate'] = df['Email'].duplicated(keep=False)\n",
    "\n",
    "# Sort by 'Email' and 'Submitted at' in ascending order\n",
    "sorted_df = df.sort_values(by=['Email', 'Submitted at'], ascending=[True, True])\n",
    "\n",
    "sorted_df = sorted_df.reset_index(drop=True) # Reset index\n",
    "df = sorted_df\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare and update empty column values for duplicate rows so that we retain all action taker's entered values\n",
    "# Then drop the rows that are unneeded duplicates\n",
    "\n",
    "indices_to_drop = []\n",
    "\n",
    "for x in range(len(df) - 1):  # Iterate until the second last row\n",
    "\tif (df.loc[x, 'Email'] == df.loc[x + 1, 'Email']):\n",
    "\n",
    "\t\t# Iterate over each column starting from the 10th column (index 9 since indexing is zero-based)\n",
    "\t\tfor col in df.columns[9:]:\n",
    "\t\t\tif pd.isna(df.at[x+1, col]):  # Check if the next row's value is empty\n",
    "\t\t\t\tdf.at[x+1, col] = df.at[x, col]  # Set the value to the current row's value\n",
    "\t\n",
    "\t\tindices_to_drop.append(x)\n",
    "\t\n",
    "df = df.drop(indices_to_drop)\n",
    "\n",
    "df = df.reset_index(drop=True) # Reset index\n",
    "\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting in the 9th column, count the number of non-NaN values in each row and store in 'Response_Count'\n",
    "# However, do NOT count values in the UX columns\n",
    "# 'Response_Count' quantifies each action taker's level of engagement at the survey level\n",
    "# NOTE: We should NOT include rows of data in certain analyses if 'Response_Count' == 0\n",
    "\n",
    "# List of strings to exclude columns\n",
    "exclude_strings = ['More Info', 'Additional ideas for your comment', 'Email link not work for you']\n",
    "\n",
    "# Create a mask for columns to exclude\n",
    "exclude_mask = df.columns.str.contains('|'.join(exclude_strings))\n",
    "\n",
    "# Create a mask for columns to include (starting from the 10th column, index 9)\n",
    "include_mask = ~exclude_mask & (np.arange(len(df.columns)) >= 9)\n",
    "\n",
    "# Count non-NaN values in each row for the included columns and store in 'Response_Count'\n",
    "df['Response_Count'] = df.loc[:, include_mask].notna().sum(axis=1)\n",
    "\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cleanup of unneeded column\n",
    "df = df.drop('Is_Duplicate', axis=1)\n",
    "\n",
    "# Define file path\n",
    "# csv_path = r'D:\\Users\\peter\\PythonProjects\\PandasTest\\data\\Week 05 Wed_cleaned.csv'\n",
    "\n",
    "# Write DataFrame to CSV file\n",
    "# df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the attribute columns starting from the 7th column (index 6)\n",
    "attribute_columns = df.columns[6:]\n",
    "\n",
    "# Ensure that we have the correct columns\n",
    "# print(\"Attribute columns:\", attribute_columns)\n",
    "\n",
    "# Unpivot the DataFrame\n",
    "normalized_df = pd.melt(df, id_vars=df.columns[:6], value_vars=attribute_columns, var_name='Attribute', value_name='Value')\n",
    "\n",
    "# Convert 'Attribute' to a categorical type to maintain the order dynamically\n",
    "normalized_df['Attribute'] = pd.Categorical(normalized_df['Attribute'], categories=attribute_columns, ordered=True)\n",
    "\n",
    "# Sort by 'ID' and the categorical 'Attribute' to maintain the sequence and reset index\n",
    "normalized_df = normalized_df.sort_values(['Email', 'Attribute']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new 'Key ID' column by concatenating 'Submission ID' + a number in the format of 01, 02, 03, etc.\n",
    "\n",
    "# Create the 'Key ID' column with the formatted string\n",
    "normalized_df.insert(0, 'Key ID', normalized_df.groupby('Submission ID').cumcount() + 1)\n",
    "\n",
    "# Concatenate 'Submission ID' to the 'Key ID' two-digit number with leading zeros\n",
    "normalized_df['Key ID'] = normalized_df['Submission ID'] + normalized_df['Key ID'].apply(lambda x: f\"{x:02}\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "# print(normalized_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 'Action Number' column\n",
    "\n",
    "# Initialize an empty list to store the action numbers\n",
    "action_numbers = []\n",
    "\n",
    "# Iterate over each group in the DataFrame based on 'Submission ID'\n",
    "for _, group in normalized_df.groupby('Submission ID'):\n",
    "    # Initialize action number counter for each group\n",
    "    action_number = 1\n",
    "    \n",
    "    # Iterate through the 'Attribute' column in the group\n",
    "    for attribute in group['Attribute']:\n",
    "        # Check if the current attribute contains 'Response_Count' or 'Submitted at'\n",
    "        if 'Response_Count' in attribute or 'Submitted at' in attribute:\n",
    "            # Append None to action_numbers if condition is met\n",
    "            action_numbers.append(0)\n",
    "        else:\n",
    "            # Append the current action number to the list\n",
    "            action_numbers.append(action_number)\n",
    "            \n",
    "            # Check if the current attribute marks the end of a subgroup\n",
    "            if 'Run into issues with' in attribute:\n",
    "                # Increment the action number for the next subgroup\n",
    "                action_number += 1\n",
    "\n",
    "# Assign the list of action numbers to the new 'Action Number' column in the DataFrame\n",
    "normalized_df['Action Number'] = action_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 'Attribute Type' column\n",
    "\n",
    "# Define conditions and corresponding values\n",
    "conditions = [\n",
    "    normalized_df['Attribute'].str.contains('Response_Count', case=False, na=False),\n",
    "    normalized_df['Attribute'].str.contains('Submitted at', case=False, na=False),\n",
    "    normalized_df['Attribute'].str.contains('More Info', case=False, na=False),\n",
    "    normalized_df['Attribute'].str.contains('Additional ideas for your comment', case=False, na=False),\n",
    "    normalized_df['Attribute'].str.contains('Email link not work for you', case=False, na=False),\n",
    "    normalized_df['Attribute'].str.contains('sign in for', case=False, na=False),\n",
    "    normalized_df['Attribute'].str.contains('sign in OR', case=False, na=False),\n",
    "    normalized_df['Attribute'].str.contains('Did you email', case=False, na=False),\n",
    "    normalized_df['Attribute'].str.contains('form email', case=False, na=False),\n",
    "    normalized_df['Attribute'].str.contains('leave a comment for', case=False, na=False),\n",
    "    normalized_df['Attribute'].str.contains('send a comment to', case=False, na=False),\n",
    "    normalized_df['Attribute'].str.contains('issues', case=False, na=False)\n",
    "]\n",
    "\n",
    "# Define corresponding Attribute Type values\n",
    "choices = [\n",
    "    'Response Count',\n",
    "    'Submitted Datetime',\n",
    "    'More Info',\n",
    "    'Action Help',\n",
    "    'Action Help',\n",
    "    'Action Verification',\n",
    "    'Action Verification',\n",
    "    'Action Verification',\n",
    "    'Action Verification',\n",
    "    'Action Verification',\n",
    "    'Action Verification',\n",
    "    'Issues'\n",
    "]\n",
    "\n",
    "# Create new column 'Attribute Type'\n",
    "normalized_df['Attribute Type'] = np.select(conditions, choices, default='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_6376\\2605610769.py:44: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  normalized_df = normalized_df.groupby(['Submission ID', 'Action Number'], as_index=False).apply(calculate_action_type).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Define a function to calculate the 'Action Type' based on the 'Attribute' column and number of rows\n",
    "def calculate_action_type(group):\n",
    "    if len(group) == 4:\n",
    "        # Define the conditions for 4-row groups\n",
    "        conditions_4 = [\n",
    "            'sign in for' in group.iloc[2]['Attribute'] if len(group) > 2 else False,\n",
    "            'sign in OR' in group.iloc[2]['Attribute'] if len(group) > 2 else False,\n",
    "            'Did you email' in group.iloc[2]['Attribute'] if len(group) > 2 else False,\n",
    "            'form email' in group.iloc[2]['Attribute'] if len(group) > 2 else False,\n",
    "            'leave a comment for' in group.iloc[2]['Attribute'] if len(group) > 2 else False,\n",
    "            'send a comment to' in group.iloc[2]['Attribute'] if len(group) > 2 else False\n",
    "        ]\n",
    "        \n",
    "        choices_4 = [\n",
    "            'Sign in', \n",
    "            'Sign in OR Written Testimony', \n",
    "            'Email', \n",
    "            'Partner Form Email', \n",
    "            'Written Testimony', \n",
    "            'Comment to District Legislator'\n",
    "        ]\n",
    "        \n",
    "        group['Action Type'] = np.select(conditions_4, choices_4, default=np.nan)\n",
    "\n",
    "    if len(group) == 5:\n",
    "        # Define the conditions for 5-row groups\n",
    "        conditions_5 = [\n",
    "            'sign in for' in group.iloc[3]['Attribute'] if len(group) > 3 else False,\n",
    "            'leave a comment for' in group.iloc[3]['Attribute'] if len(group) > 3 else False,\n",
    "            'send a comment to' in group.iloc[3]['Attribute'] if len(group) > 3 else False\n",
    "        ]\n",
    "        \n",
    "        choices_5 = [\n",
    "            'Sign in with companion bill', \n",
    "            'Written Testimony with companion bill', \n",
    "            'Comment to District Legislator with companion bill'\n",
    "        ]\n",
    "\n",
    "        group['Action Type'] = np.select(conditions_5, choices_5, default=np.nan)\n",
    "\n",
    "    return group\n",
    "\n",
    "# Group by 'Submission ID' and 'Action Number', then apply the function\n",
    "normalized_df = normalized_df.groupby(['Submission ID', 'Action Number'], as_index=False).apply(calculate_action_type).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_6376\\2758427616.py:38: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  normalized_df = normalized_df.groupby(['Submission ID', 'Action Number'], as_index=False).apply(calculate_action_completed).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the 'Action Completed' column\n",
    "\n",
    "# Define a function to apply conditions based on the 'Value' column and number of rows\n",
    "def calculate_action_completed(group):\n",
    "    # Fill NaN values with empty strings to avoid TypeError\n",
    "    group['Value'] = group['Value'].fillna('')\n",
    "    group['Action Type'] = group['Action Type'].fillna('')\n",
    "\n",
    "    # Check if the group has 4 rows\n",
    "    if len(group) == 4:\n",
    "        value_2 = group.iloc[2]['Value']\n",
    "        action_type_2 = group.iloc[2]['Action Type']\n",
    "\n",
    "        if value_2 == '':\n",
    "            group['Action Completed'] = 'Skipped'\n",
    "        elif 'Sign in OR Written Testimony' in action_type_2 and ('sign in for' in value_2 or 'and left a comment' in value_2):\n",
    "            group['Action Completed'] = 'Yes'\n",
    "        elif 'No, I did not' in value_2:\n",
    "            group['Action Completed'] = 'No'\n",
    "        elif 'Yes' in value_2:\n",
    "            group['Action Completed'] = 'Yes'\n",
    "\n",
    "    # Check if the group has 5 rows\n",
    "    elif len(group) == 5:\n",
    "        value_2 = group.iloc[2]['Value']\n",
    "        value_3 = group.iloc[3]['Value']\n",
    "\n",
    "        if value_2 == '' and value_3 == '':\n",
    "            group['Action Completed'] = 'Skipped'\n",
    "        elif 'Yes' in value_2 or 'Yes' in value_3:\n",
    "            group['Action Completed'] = 'Yes'\n",
    "        elif 'No, I did not' in value_2 and 'No, I did not' in value_3:\n",
    "            group['Action Completed'] = 'No'\n",
    "\n",
    "    return group\n",
    "\n",
    "# Group by 'Submission ID' and 'Action Number', then apply the function\n",
    "normalized_df = normalized_df.groupby(['Submission ID', 'Action Number'], as_index=False).apply(calculate_action_completed).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'Contact Type' column based on 'Attribute' column (special case when 'Action Type' = 'Sign in OR Written Testimony')\n",
    "\n",
    "def calculate_contact_type(row):\n",
    "    # Step 1: Check 'Action Verification' and 'Action Completed' values\n",
    "    if row['Attribute Type'] == 'Action Verification' and row['Action Completed'] == 'Yes':\n",
    "\n",
    "        # Step 2: Handle case for 'Action Type' = 'Sign in OR Written Testimony'\n",
    "        if row['Action Type'] == 'Sign in OR Written Testimony':\n",
    "            if 'and signed in' in row['Value']:\n",
    "                return 'Sign in'\n",
    "            elif 'and left a comment' in row['Value']:\n",
    "                return 'Written Testimony'\n",
    "        \n",
    "        # Step 3: Other conditions based on 'Attribute' values\n",
    "        elif 'sign in for' in row['Attribute']:\n",
    "            return 'Sign in'\n",
    "        elif 'email the' in row['Attribute']:\n",
    "            return 'Email'\n",
    "        elif 'leave a comment for' in row['Attribute']:\n",
    "            return 'Written Testimony'\n",
    "        elif 'send a comment to' in row['Attribute']:\n",
    "            return 'Comment to District Legislator'\n",
    "        elif 'form email' in row['Attribute']:\n",
    "            return 'Partner Form Email'\n",
    "    \n",
    "    # Default case\n",
    "    return None\n",
    "\n",
    "# Apply the function to the DataFrame to create the 'Contact Type' column\n",
    "normalized_df['Contact Type'] = normalized_df.apply(calculate_contact_type, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 'Contact' column based on 'Contact Type' and 'Attribute' columns\n",
    "\n",
    "def calculate_contact(row):\n",
    "    # Step 1: Check 'Action Verification' and 'Action Completed' values\n",
    "    if row['Attribute Type'] == 'Action Verification' and row['Action Completed'] == 'Yes':\n",
    "\n",
    "        # Step 2: Conditions based on 'Contact Type' and 'Attribute' values\n",
    "        if row['Contact Type'] == 'Sign in':\n",
    "            return 'Committee'\n",
    "        elif row['Contact Type'] == 'Written Testimony':\n",
    "            return 'Committee'\n",
    "        elif row['Contact Type'] == 'Email':\n",
    "            if 'committee members' in row['Attribute']:\n",
    "                return 'Committee'\n",
    "            elif 'legislators' in row['Attribute']:\n",
    "                return 'Specific Legislators'\n",
    "            elif 'members of the House' in row['Attribute']:\n",
    "                return 'House Democrats'\n",
    "            elif 'members of the Senate' in row['Attribute']:\n",
    "                return 'Senate Democrats'\n",
    "        elif row['Contact Type'] == 'Comment to District Legislator':\n",
    "            if 'Senator and two Representatives' in row['Attribute']:\n",
    "                return 'Senator and Representatives'\n",
    "            if 'Senator' in row['Attribute']:\n",
    "                return 'Senator'\n",
    "            elif 'two Representatives' in row['Attribute']:\n",
    "                return 'Representatives'\n",
    "        elif row['Contact Type'] == 'Partner Form Email':\n",
    "            if 'Senator and two Representatives' in row['Attribute']:\n",
    "                return 'Senator and Representatives'\n",
    "            elif 'Senator' in row['Attribute']:\n",
    "                return 'Senator'\n",
    "            elif 'two Representatives' in row['Attribute']:\n",
    "                return 'Representatives'\n",
    "    \n",
    "    # Default case\n",
    "    return None\n",
    "\n",
    "# Apply the function to the DataFrame to create the 'Contact' column\n",
    "normalized_df['Contact'] = normalized_df.apply(calculate_contact, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 'Contact Position' based on 'Attribute' column\n",
    "\n",
    "def calculate_contact_position (row):\n",
    "    # Step 1: Check 'Action Verification' and 'Action Completed' values\n",
    "    if row['Attribute Type'] == 'Action Verification' and row['Action Completed'] == 'Yes':\n",
    "\n",
    "        # Step 2: Conditions based on 'Attribute' values\n",
    "        if 'pro' in row['Attribute']:\n",
    "            return 'Pro'\n",
    "        elif 'con' in row['Attribute']:\n",
    "            return 'Con'\n",
    "        elif 'other' in row['Attribute']:\n",
    "            return 'Other'\n",
    "        elif 'support' in row['Attribute']:\n",
    "            return 'Support'\n",
    "        elif 'oppose' in row['Attribute']:\n",
    "            return 'Oppose'\n",
    "        elif 'modify' in row['Attribute']:\n",
    "            return 'Other'\n",
    "\n",
    "    # Default case\n",
    "    return None\n",
    "\n",
    "# Apply the function to the DataFrame to create the 'Contact Position' column\n",
    "normalized_df['Contact Position'] = normalized_df.apply(calculate_contact_position, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 'Contact Count' column based on 'Contact Type' and 'Attribute' columns\n",
    "def calculate_contact_count(row):\n",
    "    # Step 1: Check 'Action Verification' and 'Action Completed' values\n",
    "    if row['Attribute Type'] == 'Action Verification' and row['Action Completed'] == 'Yes':\n",
    "        \n",
    "        # Step 2: Condition when 'Contact Type' is 'Email'\n",
    "        if row['Contact Type'] == 'Email':\n",
    "            # Ensure 'Attribute' is a string and extract the first number using regex\n",
    "            if isinstance(row['Attribute'], str):\n",
    "                number = pd.to_numeric(pd.Series(row['Attribute']).str.extract(r'(\\d+)', expand=False)).iloc[0]\n",
    "                return number if pd.notna(number) else 0\n",
    "            return 0\n",
    "\n",
    "        # Step 3: Other conditions based on 'Contact Type' and 'Attribute' values\n",
    "        elif row['Contact Type'] == 'Sign in':\n",
    "            return 1\n",
    "        elif row['Contact Type'] == 'Written Testimony':\n",
    "            return 1\n",
    "        elif row['Contact Type'] == 'Comment to District Legislator':\n",
    "            if row['Contact'] == 'Senator and Representatives':\n",
    "                return 3\n",
    "            elif row['Contact'] == 'Representatives':\n",
    "                return 2\n",
    "            elif row['Contact'] == 'Senator':\n",
    "                return 1\n",
    "        elif row['Contact Type'] == 'Partner Form Email':\n",
    "            if row['Contact'] == 'Senator and Representatives':\n",
    "                return 3\n",
    "            elif row['Contact'] == 'Representatives':\n",
    "                return 2\n",
    "            elif row['Contact'] == 'Senator':\n",
    "                return 1\n",
    "    \n",
    "    # Default case\n",
    "    return None\n",
    "\n",
    "# Apply the function to the DataFrame to create the 'Contact Count' column\n",
    "normalized_df['Contact Count'] = normalized_df.apply(calculate_contact_count, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 'Bill Number' from the 'Attribute' column\n",
    "\n",
    "# Combine the HB and SB regex patterns into one\n",
    "bill_pattern = r'(HB \\d+\\.\\d*|HB \\d+|SB \\d+\\.\\d*|SB \\d+)\\?'\n",
    "\n",
    "# Function to extract and assign bill numbers\n",
    "def extract_and_assign_bill_numbers(df):\n",
    "    # Create a mask for rows where 'Attribute Type' is 'Action Verification'\n",
    "    action_verif_mask = df['Attribute Type'] == 'Action Verification'\n",
    "\n",
    "    # Initialize the 'Bill Number' column explicitly as object type (string)\n",
    "    df['Bill Number'] = pd.Series(np.nan, index=df.index, dtype=object)\n",
    "\n",
    "    # Extract the bill numbers where the mask is True using a single regex\n",
    "    df.loc[action_verif_mask, 'Bill Number'] = df.loc[action_verif_mask, 'Attribute'].str.extract(bill_pattern)[0]\n",
    "\n",
    "    # Forward fill the extracted bill numbers within each 'Submission ID' and 'Action Number' group\n",
    "    df['Bill Number'] = df.groupby(['Submission ID', 'Action Number'])['Bill Number'].transform('first')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to the entire dataframe at once\n",
    "normalized_df = extract_and_assign_bill_numbers(normalized_df)\n",
    "\n",
    "# Reset the index if needed\n",
    "normalized_df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path\n",
    "csv_path = r'D:\\Users\\peter\\PythonProjects\\PandasTest\\data\\Week 05 Wed_normalized.csv'\n",
    "\n",
    "# Write DataFrame to CSV file\n",
    "normalized_df.to_csv(csv_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
